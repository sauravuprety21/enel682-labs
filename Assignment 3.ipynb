{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 3: Non-Linear Models and Validation Metrics (38 total marks)\n",
    "### Due: March 5 at 11:59pm\n",
    "\n",
    "### Name: Saurav Uprety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses non-linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2d2c3",
   "metadata": {},
   "source": [
    "## Part 1: Regression (14.5 marks)\n",
    "\n",
    "For this section, we will be continuing with the concrete example from yellowbrick. You will need to compare these results to the results from the previous assignment. Please use the results from the solution if you were unable to complete Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (0.5 marks)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the concrete dataset into the feature matrix `X` and target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Import concrete dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_concrete\n",
    "\n",
    "X, y = load_concrete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0 marks)\n",
    "\n",
    "Data processing was completed in the previous assignment. No need to repeat here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import the Decision Tree, Random Forest and Gradient Boosting Machines regression models from sklearn\n",
    "2. Instantiate the three models with `max_depth = 5`. Are there any other parameters that you will need to set?\n",
    "3. Implement each machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the average training and validation accuracy using mean squared error with cross-validation. To do this, you will need to set `scoring='neg_mean_squared_error'` in your `cross_validate` function and negate the results (multiply by -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3f7a8",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: DT, RF and GB\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdc93a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=5, random_state=0)\n",
    "\n",
    "# random forest can specify number of trees (n_estimator) hyper param.\n",
    "# can also specify max features - since its regression task, use num of features by default\n",
    "# don't need max_features, or min_samples_leaf as pre-pruning done with max_depth\n",
    "forest_reg = RandomForestRegressor(max_depth=5, random_state=0)\n",
    "\n",
    "# Can also specify learning rate hyper param. as well as n_estimator\n",
    "gb_reg = GradientBoostingRegressor(max_depth=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Gradient Boosted Machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CV Training R2</th>\n",
       "      <td>47.918561</td>\n",
       "      <td>32.055432</td>\n",
       "      <td>3.73927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV Validation R2</th>\n",
       "      <td>163.087775</td>\n",
       "      <td>156.404972</td>\n",
       "      <td>99.360259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Decision Tree Random Forest Gradient Boosted Machine\n",
       "CV Training R2       47.918561     32.055432                  3.73927\n",
       "CV Validation R2    163.087775    156.404972                99.360259"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.DataFrame(columns=['Decision Tree', 'Random Forest', 'Gradient Boosted Machine'],\n",
    "                   index=['CV Training R2', 'CV Validation R2'])\n",
    "\n",
    "\n",
    "for i, regressor in enumerate([tree_reg, forest_reg, gb_reg]):\n",
    "    scores_ = cross_validate(regressor, X, y, cv=5, return_train_score=True, scoring='neg_mean_squared_error', ) \n",
    "    res.iloc[0,i] = (-1*scores_['train_score']).mean()\n",
    "    res.iloc[1,i] = (-1*scores_['test_score']).mean()\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4434bb",
   "metadata": {},
   "source": [
    "Repeat the step above to print the R2 score instead of the mean-squared error. For this case, you can use `scoring='r2'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Gradient Boosted Machine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CV Training R2</th>\n",
       "      <td>0.822887</td>\n",
       "      <td>0.881221</td>\n",
       "      <td>0.986436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV Validation R2</th>\n",
       "      <td>0.17621</td>\n",
       "      <td>0.173748</td>\n",
       "      <td>0.473701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Decision Tree Random Forest Gradient Boosted Machine\n",
       "CV Training R2        0.822887      0.881221                 0.986436\n",
       "CV Validation R2       0.17621      0.173748                 0.473701"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.DataFrame(columns=['Decision Tree', 'Random Forest', 'Gradient Boosted Machine'],\n",
    "                   index=['CV Training R2', 'CV Validation R2'])\n",
    "\n",
    "\n",
    "for i, regressor in enumerate([tree_reg, forest_reg, gb_reg]):\n",
    "    scores_ = cross_validate(regressor, X, y, cv=5, return_train_score=True, scoring='r2',) \n",
    "    res.iloc[0,i] = np.mean(scores_['train_score'])\n",
    "    res.iloc[1,i] = np.mean(scores_['test_score'])\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5257a98",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do these results compare to the results using a linear model in the previous assignment? Use values.\n",
    "1. Out of the models you tested, which model would you select for this dataset and why?\n",
    "1. If you wanted to increase the accuracy of the tree-based models, what would you do? Provide two suggestions.\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compared to linear models, the results are worse. Although the training R2 scores are much better for the non linear methods, 0.823, 0.881, 0.986 for decision trees, random forest, and gradient boost machines, respectively, compared to 0.611 for linear regression. The validatioon scores for the non-linear methods are significantly lower - 0.176, 0.173, 0.474 (decision trees, random forest, gradient boost machines) compared to 0.623 for linear regression. Similar trends are observed for the mean squared error. This discrepancy is most likely due to the differences in training size, and the inability to non-linear methods to extrapolate. Since cross-validation is used, with the default value of 5 folds. The training size for each cross-validation step is much smaller compared to the training size of the linear regression dataset (20 % comapred to 75 %). Hence, the models are in the high-variance region (left) of the learning curve. Finally, the tree-based regression methods have limited ability to extrapolate beyond their training dataset, this coupled with the limited training dataset could explain the discrepancy between the validation and training accuracies.\n",
    "2. Out of the models, I would use GBM for this dataset. GBM has the best perforamce, 0.990 and 0.474 training and test accuracies, respectively. GBM can also be highly tuned with the learning rate and n estimators hyperparameters. \n",
    "3. For the GBM and RF, tuning the hyperparameters - tree estimators, and for GBM the learning rate should help increase the accuracy. However, the obious limiting factor here is the learning size, as the training and validation scores are large, so increasing the dataset size, or the number of samples, should also increase accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7d3ca",
   "metadata": {},
   "source": [
    "I wrote this code myself, I utilized the sklearn documentation for some details such as the intilization of cross_validate and the different regressor. I also used the *Introduction to Machine Learning with Python* textbook along with the course notes for understanding of decision trees/ensemble methods. I completed the order in the steps perscribed. The results of this questions were challenging to understand, the large difference in training vs. validation scores caused significant doubt. I also saw that if the dataset is split into test/train, better results could be achevied. However, since we are cross validating, and also not doing any prediction with the model, I could not justify doing the test train split. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 2: Classification (18.5 marks)\n",
    "\n",
    "You have been asked to develop code that can help the user classify different wine samples. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (2 marks)\n",
    "\n",
    "The data used for this task can be downloaded from UCI: https://archive.ics.uci.edu/dataset/109/wine\n",
    "\n",
    "Use the pandas library to load the dataset. You must define the column headers if they are not included in the dataset \n",
    "\n",
    "You will need to split the dataset into feature matrix `X` and target vector `y`. Which column represents the target vector?\n",
    "\n",
    "Print the size and type of `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is of type <class 'pandas.core.frame.DataFrame'> and size (178, 13)\n",
      "y is of type <class 'pandas.core.frame.DataFrame'> and size (178, 1)\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Import wine dataset\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "wine = fetch_ucirepo(id=109) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = wine.data.features \n",
    "y = wine.data.targets \n",
    "\n",
    "print(f\"X is of type {type(X)} and size {X.shape}\")\n",
    "print(f\"y is of type {type(y)} and size {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 109, 'name': 'Wine', 'repository_url': 'https://archive.ics.uci.edu/dataset/109/wine', 'data_url': 'https://archive.ics.uci.edu/static/public/109/data.csv', 'abstract': 'Using chemical analysis to determine the origin of wines', 'area': 'Physics and Chemistry', 'tasks': ['Classification'], 'characteristics': ['Tabular'], 'num_instances': 178, 'num_features': 13, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1992, 'last_updated': 'Mon Aug 28 2023', 'dataset_doi': '10.24432/C5PC7J', 'creators': ['Stefan Aeberhard', 'M. Forina'], 'intro_paper': {'title': 'Comparative analysis of statistical pattern recognition methods in high dimensional settings', 'authors': 'S. Aeberhard, D. Coomans, O. Vel', 'published_in': 'Pattern Recognition', 'year': 1994, 'url': 'https://www.semanticscholar.org/paper/83dc3e4030d7b9fbdbb4bde03ce12ab70ca10528', 'doi': '10.1016/0031-3203(94)90145-7'}, 'additional_info': {'summary': 'These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. \\r\\n\\r\\nI think that the initial data set had around 30 variables, but for some reason I only have the 13 dimensional version. I had a list of what the 30 or so variables were, but a.)  I lost it, and b.), I would not know which 13 variables are included in the set.\\r\\n\\r\\nThe attributes are (dontated by Riccardo Leardi, riclea@anchem.unige.it )\\r\\n1) Alcohol\\r\\n2) Malic acid\\r\\n3) Ash\\r\\n4) Alcalinity of ash  \\r\\n5) Magnesium\\r\\n6) Total phenols\\r\\n7) Flavanoids\\r\\n8) Nonflavanoid phenols\\r\\n9) Proanthocyanins\\r\\n10)Color intensity\\r\\n11)Hue\\r\\n12)OD280/OD315 of diluted wines\\r\\n13)Proline \\r\\n\\r\\nIn a classification context, this is a well posed problem with \"well behaved\" class structures. A good data set for first testing of a new classifier, but not very challenging.           ', 'purpose': 'test', 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'All attributes are continuous\\r\\n\\t\\r\\nNo statistics available, but suggest to standardise variables for certain uses (e.g. for us with classifiers which are NOT scale invariant)\\r\\n\\r\\nNOTE: 1st attribute is class identifier (1-3)', 'citation': None}}\n",
      "                            name     role         type demographic  \\\n",
      "0                          class   Target  Categorical        None   \n",
      "1                        Alcohol  Feature   Continuous        None   \n",
      "2                      Malicacid  Feature   Continuous        None   \n",
      "3                            Ash  Feature   Continuous        None   \n",
      "4              Alcalinity_of_ash  Feature   Continuous        None   \n",
      "5                      Magnesium  Feature      Integer        None   \n",
      "6                  Total_phenols  Feature   Continuous        None   \n",
      "7                     Flavanoids  Feature   Continuous        None   \n",
      "8           Nonflavanoid_phenols  Feature   Continuous        None   \n",
      "9                Proanthocyanins  Feature   Continuous        None   \n",
      "10               Color_intensity  Feature   Continuous        None   \n",
      "11                           Hue  Feature   Continuous        None   \n",
      "12  0D280_0D315_of_diluted_wines  Feature   Continuous        None   \n",
      "13                       Proline  Feature      Integer        None   \n",
      "\n",
      "   description units missing_values  \n",
      "0         None  None             no  \n",
      "1         None  None             no  \n",
      "2         None  None             no  \n",
      "3         None  None             no  \n",
      "4         None  None             no  \n",
      "5         None  None             no  \n",
      "6         None  None             no  \n",
      "7         None  None             no  \n",
      "8         None  None             no  \n",
      "9         None  None             no  \n",
      "10        None  None             no  \n",
      "11        None  None             no  \n",
      "12        None  None             no  \n",
      "13        None  None             no  \n"
     ]
    }
   ],
   "source": [
    "# metadata \n",
    "print(wine.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(wine.variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908942db",
   "metadata": {},
   "source": [
    "Print the first five rows of the dataset to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "033dded2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malicacid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity_of_ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>0D280_0D315_of_diluted_wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  Total_phenols  \\\n",
       "0    14.23       1.71  2.43               15.6        127           2.80   \n",
       "1    13.20       1.78  2.14               11.2        100           2.65   \n",
       "2    13.16       2.36  2.67               18.6        101           2.80   \n",
       "3    14.37       1.95  2.50               16.8        113           3.85   \n",
       "4    13.24       2.59  2.87               21.0        118           2.80   \n",
       "\n",
       "   Flavanoids  Nonflavanoid_phenols  Proanthocyanins  Color_intensity   Hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   0D280_0D315_of_diluted_wines  Proline  \n",
       "0                          3.92     1065  \n",
       "1                          3.40     1050  \n",
       "2                          3.17     1185  \n",
       "3                          3.45     1480  \n",
       "4                          2.93      735  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a3c2f",
   "metadata": {},
   "source": [
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88413313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alcohol                         0\n",
       "Malicacid                       0\n",
       "Ash                             0\n",
       "Alcalinity_of_ash               0\n",
       "Magnesium                       0\n",
       "Total_phenols                   0\n",
       "Flavanoids                      0\n",
       "Nonflavanoid_phenols            0\n",
       "Proanthocyanins                 0\n",
       "Color_intensity                 0\n",
       "Hue                             0\n",
       "0D280_0D315_of_diluted_wines    0\n",
       "Proline                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db81660",
   "metadata": {},
   "source": [
    "How many samples do we have of each type of wine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f38fdb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "2    71\n",
       "1    59\n",
       "3    48\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "y['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `SVC` and `RandomForestClassifier` from sklearn\n",
    "2. Instantiate models as `SVC()` and `RandomForestClassifier(max_depth = 2)`\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0870b0d2",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model \n",
    "\n",
    "Calculate the average training and validation accuracy using `cross_validate` for the two different models listed in Step 3. For this case, use `scoring='accuracy'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bbd83",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "#### Step 5.1: Compare Models\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy\n",
    "2. Add the training and validation accuracy for each model to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5.1\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "rf_mdl = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "svc_mdl = SVC(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>SVC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Accuracy</th>\n",
       "      <td>0.985965</td>\n",
       "      <td>0.703743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.972063</td>\n",
       "      <td>0.663492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Random Forest       SVC\n",
       "Training Accuracy        0.985965  0.703743\n",
       "Validation Accuracy      0.972063  0.663492"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pd.DataFrame(columns=['Random Forest', 'SVC'],\n",
    "                   index=['Training Accuracy', 'Validation Accuracy'])\n",
    "\n",
    "\n",
    "for i, regressor in enumerate([rf_mdl, svc_mdl]):\n",
    "    scores_ = cross_validate(regressor, X, y['class'], cv=5, return_train_score=True, scoring='accuracy') \n",
    "    res.iloc[0,i] = np.mean(scores_['train_score'])\n",
    "    res.iloc[1,i] = np.mean(scores_['test_score'])\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e8e832",
   "metadata": {},
   "source": [
    "#### Step 5.2: Improve Model Performance\n",
    "\n",
    "As stated in class, support vector machines require additional pre-processing compared to tree-based models. Write the code below to test three different scaling methods, `MinMaxScaler`, `StandardScaler` and `RobustScaler`. For this case, use the same cross-validation method mentioned in the previous step. Print the training and validation accuracy results in a table. Use the default parameters for the `SVC`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "283a379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Test different scaling methods for the SVM model\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "res = pd.DataFrame(columns=['MinMaxScaler','StandardScale', 'RobustScaler'], index=['Training Accuracy', 'Validation Accuracy'])\n",
    "\n",
    "\n",
    "for i, Scaler in enumerate([MinMaxScaler, StandardScaler, RobustScaler]):\n",
    "    scaler = Scaler()\n",
    "    scaler.fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    scores_ = cross_validate(svc_mdl, X_scaled, y['class'], cv=5, return_train_score=True, scoring='accuracy') \n",
    "    res.iloc[0,i] = np.mean(scores_['train_score'])\n",
    "    res.iloc[1,i] = np.mean(scores_['test_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinMaxScaler</th>\n",
       "      <th>StandardScale</th>\n",
       "      <th>RobustScaler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Accuracy</th>\n",
       "      <td>0.995794</td>\n",
       "      <td>0.998601</td>\n",
       "      <td>0.998601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <td>0.977619</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.972222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    MinMaxScaler StandardScale RobustScaler\n",
       "Training Accuracy       0.995794      0.998601     0.998601\n",
       "Validation Accuracy     0.977619      0.983333     0.972222"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf319621",
   "metadata": {},
   "source": [
    "### Questions (7 marks)\n",
    "1. How do the training and validation accuracy change depending on the method used (without scaling)? Were either of these models a good fit for the data?\n",
    "1. What are two reasons why a support vector machines model might not work as well as a tree-based model?\n",
    "1. How did each scaler perform compared to the unscaled results? Was there a significant difference in the performance of the scalers comparatively? Explain with values.\n",
    "1. How did the results for the scaled SVM model compare to the random forest model? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eba49a",
   "metadata": {},
   "source": [
    "1. The training and validation accuracies for both RF, and SVC (without scaling) are comparable. This means that, we are not in the high variance region of the learning curve, and increase in training data size will not improve results. The random forest performed exceptionally well, with training and validation scores of 0.996 and 0.978, respectively. However, the SVC appears to have underfit, with training and validation scores of 0.704 and 0.663\n",
    "1.  SVM is very sensitive to differences in scales of features, whereas tree-based models are not impacted this. In this case, where the features have vastly different scales, the tree-based models would outperform SVM. SVM also requires careful tuning of the gamma and C parameters, incorrect tuning could also lead to SVM having inferior performance than tree-based methods\n",
    "1.  Scaling brings incredible improvements in the results for SVC, the cross validation, training vs test scores improved from (0.704, 0.663) to (0.9958, 0.978), (0.999, 0.983), (0.998, 0.972) for unscaled, min/max scaled, standard scaled, and robust scaled, respectively. All of the scaled models appear to be a good fit for the data, as the training and validation R2 scores are near 1. Out of the scaled results, standard scaling appears to have marginally better results, as the gap between training and validation accuracies is the ssmallest. \n",
    "1. Results for the scaled models are slightly better than the random forest model. However, given that random forest is computationally less expensive in predicting, it might be the better alternative for a larger dataset, despite the slightly lower scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ff8ae",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e837da",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE - BE SPECIFIC*\n",
    " \n",
    "For getting acces to the UCI wine data, I used the example code provided in the datasets description [1]. The rest of the code, I refered to the example notebooks provided in the class. I also learned about SVM and Random Forest for the *Introduction to Machine Learning with Python* textbook. I completed the questions in order and did not have any challenges. I went first learned about the methods using the text book and then went through the relevant class examples before attempting the assignment. Going through the content in this order made the assignment less challenging. \n",
    "\n",
    "[1]Aeberhard,Stefan and Forina,M.. (1991). Wine. UCI Machine Learning Repository. https://doi.org/10.24432/C5PC7J."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7358d",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad054d9",
   "metadata": {},
   "source": [
    "In class we talked about how, tree-based methods and ensembles are some of the most powerful and widely used methods for both regression and classification. However, in here we saw that tree-based methods can also be quite limited in their regression capabilites as they have the tendency to overfit. For example, in part 1, the tree-based regressors had vast differences in training vs. validation scores, which were (0.823, 0.176), (0.881, 0.174), and (0.986, 0.474) for decision-tree, random-forest, and gradient boosted machine, respectively. <br><br>\n",
    "We also talked about the effects of difference in scale on SVC performance, and could the improvement observed when the various scalers were used to back to this point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf91320",
   "metadata": {},
   "source": [
    "Overall, I liked the assignment, as it was once again hands on. Some of the instruction could be more clear, for example, part 1-step 3.2, it asks about any other parameters we should consider. Its not clear how we are supposed to approach this question, should I just mention the other parameters, or should I play around with the other parameters to get better results, or is this just something to think about. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
